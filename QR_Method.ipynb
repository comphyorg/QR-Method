{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "QR-Method.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH9_eKEjbPOP"
      },
      "source": [
        "\n",
        "# **<center>QR Method**\n",
        "\n",
        "\n",
        "The QR method is the most-used algorithm to compute all the eigenvalues of a matrix (fulfilling some conditions) due to its robustness. The masterminds behind this algorithm are John G. F. Francis and Vera N. Kublanovskaya, working independently.<br> \n",
        "\n",
        "Once you convince yourself that you will need to find eigenvalues and eigenvectors, you can move forward to compute them. There is a class of matrices whose eigenvalues are [easily computable](https://math.libretexts.org/Bookshelves/Linear_Algebra/Book%3A_Linear_Algebra_(Schilling_Nachtergaele_and_Lankham)/07%3A_Eigenvalues_and_Eigenvectors/7.05%3A_Upper_Triangular_Matrices) through iterative methods. These are **upper-triangular matrices** which have a general form of \n",
        "<br><br>\n",
        "$$ A = \n",
        "\\begin{bmatrix}\n",
        "a_{11} & a_{12} & a_{13} & a_{14} & a_{15}  & \\cdots  & a_{1n}\\\\\n",
        "0      & a_{22} & a_{23} & a_{24} & a_{25}  & \\cdots  & a_{2n}\\\\\n",
        "0      &  0     & a_{33} & a_{34} & a_{35}  & \\cdots  & a_{3n}\\\\\n",
        "0      & 0      & 0      & a_{44} & a_{45}  & \\cdots  & a_{4n}\\\\\n",
        "0      & 0      & 0      & 0      & a_{55}  & \\cdots  & a_{5n}\\\\\n",
        "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots  &\\ddots   & \\vdots     \\\\\n",
        "0      & 0      &  0     & 0      & 0      & \\cdots   & a_{nn}\n",
        "\\end{bmatrix}\n",
        "$$ \n",
        "\n",
        "And due to the same reason, let's first convert any matrix into an upper triangular matrix through a process called **QR Algorithm**. Notice that the transformation (from a general matrix to an upper triangular) should be such that the eigenvalues remain unchanged. \n",
        "\n",
        "## <center><u>QR Factorisation/Decomposition</u>\n",
        "\n",
        "It is proposed that for all **invertible** matrices (i.e. a square matrix with $|A| \\neq 0$), say $A$ can be expressed as  \n",
        "$$A=QR$$\n",
        "where <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $Q$ is an orthogonal matrix (i.e. $Q\\ Q^T=I$) and $R$ is an upper triangular matrix. <br>\n",
        "\n",
        "But if $A$ contains complex entries, the $Q$ should be more than just orthogonal; it should be a [unitary](https://en.wikipedia.org/wiki/Unitary_matrix) matrix (i.e. $Q\\ Q^\\dagger=I, \\ Q^\\dagger $ being transpose conjugate of $Q$). Now, this is a blessing for us since that is what we needed, an upper triangular matrix related to our matrix in some way that conserves the eigenvalues. Now, let's see how to obtain these $Q\\ \\&\\ R$ matrices corresponding to your invertible matrix $A$. \n",
        "\n",
        "### <center><u>Gram-Schmidt Ortho-Normalisation</u>\n",
        "\n",
        "You should remember our good old technique called [Gram Schmidt Orthonormalisation](http://compphy.com). This method is a standard for converting a basis set into one with all the vectors, orthonormal to each other. One should ask how this technique for obtaining the set of orthonormal vectors from the linearly independent vectors' set is related to decomposing a matrix in Q and R matrix. Let us move forward keeping this question at the back of our head;\n",
        "\n",
        "Typically, we aim to convert a set of vectors (basis set) in some other set (orthogonal set), i.e. for three dimensional vector space\n",
        "$$(\\hat{a_1},\\hat{a_2},\\hat{a_3})\\rightarrow (\\hat{u_1},\\hat{u_2},\\hat{u_3})$$\n",
        "Now, we can imagine that each column of a matrix is a column vector in its own right. Then we try to change these vectors such that they become mutually orthogonal.<br>\n",
        "To demonstrate this, assume that your matrix $A$ has column vectors $\\vec{a_1},\\ \\vec{a_2},\\ \\vec{a_3}$. Now, if we obtain a set of orthonormal basis using gram-schmidt process and call them $ \\hat{e_1},\\ \\hat{e_2},\\ \\hat{e_3}$. Also, we know we can write these vectors like the following which is the essence of Gram-Schmidt Process.  \n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "\\vec{a_1} = (\\vec{a_1}.\\hat{e_1})\\hat{e_1} \\\\\n",
        "\\vec{a_2} = (\\vec{a_2}.\\hat{e_1})\\hat{e_1}+(\\vec{a_2}.\\hat{e_2})\\hat{e_2} \\\\\n",
        "\\vec{a_3} = (\\vec{a_3}.\\hat{e_1})\\hat{e_1}+(\\vec{a_3}.\\hat{e_2})\\hat{e_2}+(\\vec{a_3}.\\hat{e_3})\\hat{e_3} \\\\\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "If you recall that matrix multiplication includes several dot products (loosely speaking as in a column is multiplied by a row to give an entry of matrix). We can write the above set of equations as single matrix multiplication. It's better if you see it yourself, considering each equation as rows of $A^T$. \n",
        "\n",
        "Therefore if we let $$R^T = \\begin{bmatrix}\n",
        "(\\vec{a_1}.\\hat{e_1}) &0 & 0 \\\\\n",
        "(\\vec{a_2}.\\hat{e_1})     & (\\vec{a_2}.\\hat{e_2}) & 0 \\\\\n",
        "(\\vec{a_3}.\\hat{e_1})    &  (\\vec{a_3}.\\hat{e_2})    & (\\vec{a_3}.\\hat{e_3})\n",
        "\\end{bmatrix} $$ and $$ Q^T = \\begin{bmatrix} \\hat{e_1} \\\\ \\hat{e_2} \\\\ \\hat{e_3} \\end{bmatrix}$$\n",
        "\n",
        "then the set of equations can be written as following multiplication \n",
        "$$A^T = R^T Q^T \\\\\n",
        "\\implies A=QR $$\n",
        "\n",
        "Thus, you have *factorised* a matrix $A$ in two different matrix R and Q. Now, it turns out that these matrices Q and R follow all the properties you asked for. The matrix $Q$ is orthogonal ($Q^T = Q^{-1}$) and, the matrix $R$ is an upper triangular matrix, as you can see in the 3-dimensional example. For more detailed versions with more rigour, you can go through the references. \n",
        "\n",
        "---\n",
        "\n",
        "## <center><u>QR Algorithm/Iteration</u>\n",
        "\n",
        "Now, our hard work will get paid off. We are going to use the matrices $Q$ and $R$ for finding the eigenvalues and eigenvectors. This method is called **QR iteration** as we iteratively and decompose. \n",
        "<br>\n",
        "In this process we start with factorising a matrix A_1 as $$ A_1 = Q_1R_1$$ Then we define $$ A_2 = R_1Q_1$$ and factorise it further $$A_2 = Q_2R_2$$Continuing this process: for $k \\geq 1 $ \n",
        "\n",
        "$$ A_k = Q_k R_k $$  $$  A_{k+1} = R_k Q_k  = (Q^{-1}_k A_k )Q_k = Q^T_k A_k Q_k $$\n",
        "\n",
        "You can continue substituting like this which will fetch you $$A_{k+1} = \\big(Q^T_kQ^T_{k-1}Q^T_{k-2} \\cdots Q^T_2Q^T_1 \\big) A_1 \\big(Q_1 Q_2 \\cdots Q_{k-1} Q_k\\big)$$ This can be squished by writing $P = \\big(Q_1 Q_2 \\cdots Q_{k-1} Q_k\\big)$ and hence becomes $$A_{k+1} = P^T A_1 P$$\n",
        "After some specific $k$ you cook an **upper triangular matrix $A_{k+1}$** whose diagonal entries have **converged to eigenvalues of matrix $A$**. Technically this is called **Schur form** of $A_1$. From the previous equation you can tell that $A_{k+1}\\ \\&\\ A_1$ are [similar matrices](https://en.wikipedia.org/wiki/Matrix_similarity).  Consequently, $A_1, A_2, A_3, \\cdots $, all have same eigenvalues. Thus our only task is to find a valid $P$ and you'll get your promised eigenvalues. \n",
        "\n",
        "<br>\n",
        "Again, decomposing $A_{k+1}$ would give you a $Q$ whose column entries would the eigenvectors of $A_1$. You can go through these links for more rigour \"Numerical Linear Algebra Treatment\" if you are not feeling overwhelmed with this surge of information. I would prefer to go there if you are not satisfied with the amount of mathematics used.\n",
        "\n",
        "## **<center>Further Reading**\n",
        "1. [https://en.wikipedia.org](https://en.wikipedia.org/wiki/QR_algorithm)\n",
        "\n",
        "2. [MathThebeautiful](https://www.youtube.com/watch?v=1xcSttdeHFg)\n",
        "\n",
        "3. [http://pi.math.cornell.edu/](http://pi.math.cornell.edu/~web6140/TopTenAlgorithms/QRalgorithm.html)\n",
        "\n",
        "## Steps for Writing the Code\n",
        "\n",
        "1. You get a matrix of a certain order.\n",
        "2. Using Gram-Schmidt's process, you construct the matrix R.\n",
        "3. During the process, you'll also get all the dot products for Q.\n",
        "4. After decomposition, you'll recursively define a new matrix and decompose it until the diagonal entries stop to change very significantly (Convergence depends on your error tolerance). \n",
        "5. Factorize the latest matrix and appoint the diagonal entries as eigenvalues, as well as, appoint column entries as their corresponding eigenvectors. \n",
        "\n",
        "You should be wondering why we did not use the P matrix? Since we need to check the convergence iteratively, we can't use P directly but follow the given procedure. \n",
        "\n",
        "<br> \n",
        "<tt> You are encouraged to read up all the numpy functions we are gonna use. </tt>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3_V1VWkbPOS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4589febf-46fe-4697-8f77-d2ff8d159f4f"
      },
      "source": [
        "#importing the libraries \n",
        "import numpy as np\n",
        "import cmath as m\n",
        "\n",
        "\n",
        "################################### QR section ###########################################\n",
        "\n",
        "################################### User section #########################################\n",
        "# input your matrix here please\n",
        "# here, we have given a generalised code for complex matrices too\n",
        "\n",
        "Mat=np.array([2,3,5,3,2,4,5,4,2],'complex')\n",
        "Dim=int(m.sqrt(len(Mat)).real)\n",
        "Mat=np.reshape(Mat,(Dim,Dim))\n",
        "eig=np.zeros(Dim,'complex')\n",
        "Eig=np.identity(Dim,'complex')\n",
        "\n",
        "# you can try to print out the variables at each step to see what they are doing\n",
        "\n",
        "#################### QR factorisation through gram-schimdt orthonormalisation ############### \n",
        "def qrfacg(arr):\n",
        "    \n",
        "    # defining the matrix for manipulation \n",
        "    Q = np.array([arr[i][j] for i in range(Dim) for j in range(Dim)]).reshape(Dim, Dim)  # doing a copy real matrix/ one can simply use Q = np.copy(arr)\n",
        "    R = np.zeros(Dim**2,'complex').reshape(Dim, Dim)\n",
        "\n",
        "    # GRAM_SCHIMDT \n",
        "    for i in range(Dim):\n",
        "\n",
        "        temp = Q[:,i]  # picking a vector\n",
        "        \n",
        "        # the repeated subtraction\n",
        "        for j in range(i):\n",
        "            temp -= Q[:,j]*( np.vdot (Q[:,i], Q[:,j]) )            # Gram-Schmidt Rotation\n",
        "            \n",
        "        # the normalisation\n",
        "        temp /=(m.sqrt(np.vdot(temp,temp)))\n",
        "\n",
        "        # component matrix\n",
        "        for k in range(Dim-1,i-1,-1):\n",
        "            R[i][k] = np.vdot(arr[:, k], temp)                # R matrix is collection of older vectors in new basis\n",
        "\n",
        "        # Notice, you can simply use temp for Q matrix\n",
        "        Q[:,i] = temp\n",
        "    return Q,R\n",
        "\n",
        "## Factorised Matrix\n",
        "#print(qrfacg(Mat))\n",
        "\n",
        "########################################### QR ALGORITHM ########################################\n",
        "#################################################################################################\n",
        "\n",
        "# intialising the algo\n",
        "Qk,Rk = qrfacg(Mat)\n",
        "while(True):  \n",
        "# A do-while loop in python\n",
        "\n",
        "    # first matrix\n",
        "    Ak = np.matmul(Qk,Rk)\n",
        "    # second matrix\n",
        "    Akk = np.matmul(Rk,Qk)\n",
        "\n",
        "    # Redefining for current matrix\n",
        "    Qk,Rk = qrfacg(Akk)\n",
        "\n",
        "    dig=abs(np.diagonal(Ak)-np.diagonal(Akk))\n",
        "\n",
        "    ####### Convergence test\n",
        "    if(dig.all()<= 0.00001+0.0001j): \n",
        "      eigval  = np.diagonal(Akk)\n",
        "      print(\"The eigenvalues of matrix are \")\n",
        "      print(eigval)\n",
        "      print(\"The corresponding eigenvectors of matrix are \")\n",
        "      print(Qk.transpose())\n",
        "      break\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The eigenvalues of matrix are \n",
            "[10.05581036+0.j -3.18026779+0.j -0.87554257+0.j]\n",
            "The corresponding eigenvectors of matrix are \n",
            "[[ 1.00000000e+00+0.j  4.27506809e-09+0.j  4.76834764e-18+0.j]\n",
            " [ 4.27506809e-09+0.j -1.00000000e+00+0.j -9.75096981e-10+0.j]\n",
            " [ 5.99741651e-19+0.j  9.75096981e-10+0.j -1.00000000e+00+0.j]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}